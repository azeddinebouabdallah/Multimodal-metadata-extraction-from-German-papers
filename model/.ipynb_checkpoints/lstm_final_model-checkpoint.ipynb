{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5260546e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module): \n",
    "    # This NLP part Will consist of two bidirectional lstm layers and it's output is \n",
    "    # determined by the LSTM's last hidden states or output vectors.\n",
    "\n",
    "    # This will take as an input a sequence of words and output the last hidden layer\n",
    "    # the last hidden states of 2-layer bidirectional LSTM will be the input of the last multimodel network \n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim = 256, layer_dim =2, output_dim = 10):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        #Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim # maybe set this to 256\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building the LSTM \n",
    "        # batch_first = True causes the input/output to be of shape 3D (batch_dim, seq_dim, feature_dim) \n",
    "        # output will be the same dim as the hidden dim\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, layer_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # self.layer_dim * 2. because we have one going forwards and another going backwards\n",
    "        h0 = torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim)\n",
    "        \n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 =  torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # We suppose we are conducting a 28 time steps In case of using \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm1(x, (h0.detach(), c0.detach()))\n",
    "                \n",
    "        # out = self.fc(out.view(out.size(0), -1))\n",
    "          \n",
    "        # Without the activation function, out will contain the last hidden layer.\n",
    "        # This could be obtianed from hn[-1] as well.\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 256, 100, 256 if we have (input dim = 100 and hidden dim = 100)\n",
    "        # out[:, -1, :] => 256, 256 --> because we just want the last time step hidden states\n",
    "        #out = out[:, -1, :] # without an activation function\n",
    "\n",
    "        # now our: out.size() --> 256, 10 (if output dimension is equal to 10)\n",
    "        #return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 embedding size\n",
    "# word feature vector = [title score nlp, title score image vision, abstract score nlp, ...etc]\n",
    "# target = [1.0 (title) , 0, 0 ...etc] as an example\n",
    "model = BiLSTM(20)\n",
    "\n",
    "\n",
    "#loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./nlp_output/ssoar_datasetssoar-2008-schmidt-whats_wrong_with_the_concept.pickle', 'rb') as f:\n",
    "    nlp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_output(filename):\n",
    "    with open('./vision_output/{0}'.format(filename), 'rb') as f:\n",
    "        img = pickle.load(f)\n",
    "    for txt in img:\n",
    "        words = []\n",
    "        vector = txt[1]\n",
    "        for word in txt[0].split(' '):\n",
    "            words.append([word, vector])\n",
    "\n",
    "    words = np.array(words)\n",
    "\n",
    "    with open(\"processed_vision_output/{0}\".format(filename), 'wb') as f:\n",
    "        pickle.dump(words, f)\n",
    "        \n",
    "for file in os.scandir('./vision_output'):\n",
    "    if (os.path.isfile(\"./nlp_output/{0}\".format(file.name))):\n",
    "        print('exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Working' array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Paper'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Series'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Research'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Network'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '1989'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) \"What's\"\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Wrong'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'With'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'the'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Concept'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'of'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Multiple'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Modernities?*'\n",
      " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Working'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Paper'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '6/2008'\n",
      " array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) 'ISSN'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '1867-2833'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Volker'\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 'H.'\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Schmidt'\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 'Department'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'Sociology'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'National'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'University'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'Singapore'\n",
      " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) 'Abstract'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'The'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'concept'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'multiple'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'modernities'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'has'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'been'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'developed'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'with'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'a'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'view'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'to'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'highlighting'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'ways'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'in'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'which'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'modern'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'societies'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'differ'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'from'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'each'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'other.'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'Other'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'sociological'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'approaches,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'by'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'contrast,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'emphasize'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'such'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) \"societies'\"\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'commonalities.'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'But'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'does'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'juxtaposition'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'convergence'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'and'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'divergence'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'in'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'form'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'a'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'mutually'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'exclusive,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'binary'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'opposition'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'really'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'make'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'sense?'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'Might'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'it'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'be'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'that'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'there'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'is'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'convergence'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'in'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'some'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'respect,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'while'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'diversity'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'persists'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'in'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'other'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'respects;'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'that'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'there'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'are'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'dimensions'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'social'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'change'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'that'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'exhibit'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'common'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'trends'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'across'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'regions'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'and'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'cultural'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'zones,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'while'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'other'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'aspects'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'social'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'life'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'show'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'remarkable'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'resilience'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'against'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'homogenization?'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'The'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'present'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'paper'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'argues'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'that'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'our'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'observation'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'convergence'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'or'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'diversity'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'might'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'be'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'less'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'a'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'matter'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'truth'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'or'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'falsity'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'than'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'an'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'artifact'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'our'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'chosen'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'methodologies.'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'Based'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'on'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'this'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'premise,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'concept'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'multiple'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'modernities'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'will'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'be'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'rejected'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'as'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'sociologically'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'meaningless,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'conceptually'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'flawed'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'and'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'empirically'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'dubious.'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'It'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'is'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'sociologically'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'meaningless'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'because'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'its'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'advocates'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'fail'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'to'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'spell'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'out'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'sufficiently'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'clearly'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'what'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'they'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'mean'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'by'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'modern'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'as'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'against'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'non-modern'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'societies;'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'it'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'is'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'conceptually'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'flawed'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'because'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'it'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'does'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'not'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'provide'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'criteria'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'for'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'distinguishing'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'theoretically'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'significant'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'from'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'insignificant'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) '(or'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'less'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'significant)'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'differences,'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'and'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'it'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'is'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'empirically'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'dubious'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'because'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'it'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'misrepresents'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'state'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) \"world's\"\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'development.'\n",
      " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]) 'Keywords'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Modernity,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'modernization,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'multiple'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'modernities,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'comparative'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'research,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'global'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'modernity'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '*'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Portions'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'this'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'paper'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'were'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'presented'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'at'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'mid-term'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'conference'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'ISA'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Research'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Committee'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'on'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Sociological'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Theory'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '(RC'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '16)'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'at'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Pusan'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'National'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'University,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Pusan,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'South'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Korea,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '23-25'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'June'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '2008,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'and'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'at'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '38th'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'World'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Congress'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'the'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'International'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Institute'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'of'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Sociology,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Budapest,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'Hungary,'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '26-30'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) 'June'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) '2008.'\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "print(nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
